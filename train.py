# -*- coding: utf-8 -*-
"""ANNs+GA_2_2-ICE_FI_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M45cd2R-ZYznQ-uKx7O5OAm-NvUpqkHu
"""

!pip install pandas openpyxl deap torch scikit-learn --quiet

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from deap import base, creator, tools, algorithms
import random
import os

# Device config
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load Dataset
file_name = '/content/drive/MyDrive/PINNs/Suction_vsCP-modified_1.xlsx'
df = pd.read_excel(file_name)



# Visualize Target Distribution
plt.figure(figsize=(6,4))
sns.histplot(df.iloc[:, -1], kde=True, color='teal')
plt.title("Collapse Potential (%) Distribution")
plt.xlabel("Collapse Potential (%)")
plt.ylabel("Count")
plt.grid(True)
plt.tight_layout()
plt.show()

# Data Prep
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()
X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

# Train-Test Split (70%-30%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.3, random_state=42)
input_dim = X.shape[1]

# Model Definition
class ANNModel(nn.Module):
    def __init__(self, input_dim, hidden1, hidden2):
        super(ANNModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.out = nn.Linear(hidden2, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.out(x)

# GA Setup
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)
toolbox = base.Toolbox()
toolbox.register("attr_h1", random.randint, 4, 64)
toolbox.register("attr_h2", random.randint, 4, 64)
toolbox.register("attr_lr", random.uniform, 0.0005, 0.01)
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_h1, toolbox.attr_h2, toolbox.attr_lr), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# Cross-validation
kf = KFold(n_splits=4, shuffle=True, random_state=42)
fold_results = []
best_model_state = None
best_score = float('inf')

for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):
    print(f"\n=== Fold {fold+1}/4 ===")

    X_tr, X_val = X_train[train_idx], X_train[val_idx]
    y_tr, y_val = y_train[train_idx], y_train[val_idx]

    X_tr_tensor = torch.tensor(X_tr, dtype=torch.float32).to(device)
    y_tr_tensor = torch.tensor(y_tr, dtype=torch.float32).view(-1, 1).to(device)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)

    def evaluate(individual):
        h1, h2 = int(individual[0]), int(individual[1])
        lr = individual[2]
        model = ANNModel(input_dim, h1, h2).to(device)
        optimizer = optim.Adam(model.parameters(), lr=lr)
        criterion = nn.MSELoss()

        for _ in range(500):
            model.train()
            optimizer.zero_grad()
            out = model(X_tr_tensor)
            loss = criterion(out, y_tr_tensor)
            loss.backward()
            optimizer.step()

        model.eval()
        with torch.no_grad():
            preds = model(X_val_tensor)
            mse = mean_squared_error(y_val, preds.cpu().numpy())
        return (mse,)

    toolbox.register("evaluate", evaluate)
    toolbox.register("mate", tools.cxTwoPoint)
    toolbox.register("mutate", tools.mutPolynomialBounded,
                     low=[4, 4, 0.0005], up=[64, 64, 0.01], eta=0.1, indpb=0.2)
    toolbox.register("select", tools.selTournament, tournsize=3)

    pop = toolbox.population(n=30)
    hof = tools.HallOfFame(1)
    algorithms.eaSimple(pop, toolbox, cxpb=0.6, mutpb=0.3, ngen=30, halloffame=hof, verbose=False)

    best_ind = hof[0]
    h1, h2, lr = int(best_ind[0]), int(best_ind[1]), best_ind[2]
    print(f"Best Params: H1={h1}, H2={h2}, LR={lr:.5f}")

    model = ANNModel(input_dim, h1, h2).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    train_losses, val_losses = [], []

    for epoch in range(300):
        model.train()
        optimizer.zero_grad()
        output = model(X_tr_tensor)
        train_loss = criterion(output, y_tr_tensor)
        train_loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            val_output = model(X_val_tensor)
            val_loss = criterion(val_output, y_val_tensor)

        train_losses.append(train_loss.item())
        val_losses.append(val_loss.item())

    if val_losses[-1] < best_score:
        best_score = val_losses[-1]
        best_model_state = model.state_dict()
        best_params = (h1, h2, lr)

    fold_results.append((train_losses, val_losses))

# Plot Train/Validation RMSE Curves
plt.figure(figsize=(10, 6))
for i, (train, val) in enumerate(fold_results):
    train_rmse = [np.sqrt(loss) for loss in train]
    val_rmse = [np.sqrt(loss) for loss in val]

    plt.plot(train_rmse, label=f"Fold {i+1} - Train", linestyle='--')
    plt.plot(val_rmse, label=f"Fold {i+1} - Val")

plt.xlabel("Epoch")
plt.ylabel("Loss (RMSE)")
plt.title("Train and Validation RMSE per Fold")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# to do here # Plot  Train/Validation RMSE Curves for the best fold

# Save Best Model
save_path = "/content/drive/MyDrive/PINNs/acl_10_7_25_best_ga_ann_model.pth"
torch.save({
    'model_state_dict': best_model_state,
    'params': best_params,
    'scaler_X': scaler_X,
    'scaler_y': scaler_y
}, save_path)
print("\n Best model saved to:", save_path)

# === Evaluation on Test Set ===
checkpoint = torch.load(save_path, map_location=device,weights_only=False)
model = ANNModel(input_dim, *checkpoint['params'][:2]).to(device)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)

with torch.no_grad():
    preds_scaled = model(X_test_tensor).cpu().numpy()
    preds        = checkpoint['scaler_y'].inverse_transform(preds_scaled.reshape(-1, 1)).flatten()
    actuals      = checkpoint['scaler_y'].inverse_transform(y_test.reshape(-1, 1)).flatten()

    mse  = mean_squared_error(actuals, preds)
    rmse = np.sqrt(mse)
    r2   = r2_score(actuals, preds)

    print(f"\n Evaluation on Test Set:")
    print(f" RMSE: {rmse:.4f}")
    print(f" R² Score: {r2:.4f}")

import copy
import matplotlib.pyplot as plt
import numpy as np
import torch

# Assumes these are already defined:
# - model, X_test, scaler_X, scaler_y, df, device

feature_names = df.columns[:-1].tolist()  # Exclude target
X_sample = X_test #[:50]  # Choose subset
X_sample_orig = scaler_X.inverse_transform(X_sample)

num_features = len(feature_names)

# Loop through every 2 features
for i in range(0, num_features, 2):
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    for j in range(2):
        feature_idx = i + j
        if feature_idx >= num_features:
            axes[j].axis('off')  # empty cell if odd number of features
            continue

        feature_name = feature_names[feature_idx]
        values_to_plot = np.linspace(
            X_sample_orig[:, feature_idx].min(),
            X_sample_orig[:, feature_idx].max(),
            50
        )

        for row in X_sample_orig:
            preds = []
            for val in values_to_plot:
                modified = copy.deepcopy(row)
                modified[feature_idx] = val
                modified_scaled = scaler_X.transform(modified.reshape(1, -1))
                input_tensor = torch.tensor(modified_scaled, dtype=torch.float32).to(device)
                with torch.no_grad():
                    pred_scaled = model(input_tensor).cpu().numpy()
                    pred = scaler_y.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()[0]
                preds.append(pred)

            axes[j].plot(values_to_plot, preds, alpha=0.3, linewidth=1)

        axes[j].set_title(f"ICE Plot: {feature_name}")
        axes[j].set_xlabel(feature_name)
        axes[j].set_ylabel("Predicted Collapse Potential (%)")
        axes[j].grid(False)

    plt.tight_layout()
    plt.show()

# Optional Visualization
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)

with torch.no_grad():
    preds_scaled = model(X_test_tensor).cpu().numpy()
    preds = checkpoint['scaler_y'].inverse_transform(preds_scaled.reshape(-1, 1)).flatten()
    actuals = checkpoint['scaler_y'].inverse_transform(y_test.reshape(-1, 1)).flatten()

    mse = mean_squared_error(actuals, preds)
    rmse = np.sqrt(mse)
    r2 = r2_score(actuals, preds)

    print(f"\n Evaluation on Test Set:")
    print(f" RMSE: {rmse:.4f}")
    print(f" R² Score: {r2:.4f}")

plt.figure(figsize=(8,6))
sns.scatterplot(x=actuals, y=preds, s=150, color="navy", edgecolor="k")
plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'r--')
plt.xlabel("Actual Collapse Potential (%)")
plt.ylabel("Predicted Collapse Potential (%)")
plt.title(f"Test Set Prediction\nRMSE={rmse:.3f}, R²={r2:.3f}")
plt.grid(True)
plt.tight_layout()
plt.show()

# === User Input Prediction ===
print("\nEnter input features (comma-separated):")
user_input = input().strip().split(',')
# Filter out empty strings before converting to float
user_input = np.array([float(i) for i in user_input if i]).reshape(1, -1)
user_scaled = checkpoint['scaler_X'].transform(user_input)
user_tensor = torch.tensor(user_scaled, dtype=torch.float32).to(device)

with torch.no_grad():
    user_pred_scaled = model(user_tensor).cpu().numpy()
    user_pred = checkpoint['scaler_y'].inverse_transform(user_pred_scaled.reshape(-1, 1)).flatten()[0]
    print(f"\n Predicted Collapse Potential: {user_pred:.2f}%")

# === User Input Prediction ===
print("\nEnter input features (comma-separated):")
user_input = input().strip().split(',')
user_input = np.array([float(i) for i in user_input]).reshape(1, -1)
user_scaled = checkpoint['scaler_X'].transform(user_input)
user_tensor = torch.tensor(user_scaled, dtype=torch.float32).to(device)

with torch.no_grad():
    user_pred_scaled = model(user_tensor).cpu().numpy()
    user_pred = checkpoint['scaler_y'].inverse_transform(user_pred_scaled.reshape(-1, 1)).flatten()[0]
    print(f"\n Predicted Collapse Potential: {user_pred:.2f}%")

# === Feature Importance using Gradient Sensitivity ===
def compute_feature_importance(model, X_tensor):
    """Compute gradient-based sensitivity importance (PyTorch version)"""
    X_tensor = X_tensor.clone().detach().requires_grad_(True)
    model.eval()

    with torch.enable_grad():
        output = model(X_tensor)
        output_sum = output.sum()  # scalar to backprop from
        output_sum.backward()
        gradients = X_tensor.grad.detach().cpu().numpy()

    avg_gradients = np.mean(np.abs(gradients), axis=0)
    importance = avg_gradients / np.sum(avg_gradients)
    return importance

# Compute and plot
feature_names = df.columns[:-1].tolist()
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)

importance_scores = compute_feature_importance(model, X_test_tensor)

# Plot
plt.figure(figsize=(10, 5))
sns.barplot(x=importance_scores, y=feature_names, palette='viridis')
plt.title('Feature Importance (Gradient-Based Sensitivity)')
plt.xlabel('Normalized Importance Score')
plt.grid(True)
plt.tight_layout()
plt.show()