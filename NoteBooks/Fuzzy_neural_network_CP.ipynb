{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xdr4ppQVxPeP",
      "metadata": {
        "id": "Xdr4ppQVxPeP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Neuro‑Fuzzy (TSK/ANFIS‑style) Regressor to predict Collapse Potential (%)\n",
        "from soil experiment features.\n",
        "Notes\n",
        "-----\n",
        "• Default #MFs per feature is M=3 → with D=6 features gives R = 3^6 = 729 rules (tractable on CPU/GPU).\n",
        "• If you have a very modest machine, reduce M to 2.\n",
        "• You can tune M, batch size, learning rate, epochs at the bottom of the script.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b_C-Db8Rxc-3",
      "metadata": {
        "id": "b_C-Db8Rxc-3"
      },
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W2eTiF4mxfL3",
      "metadata": {
        "id": "W2eTiF4mxfL3"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "DATA_CSV = \"/content/drive/MyDrive/PINNs/Suction_vsCP-modified_1.xlsx\"\n",
        "TARGET_COL = \"Collapse Potential (%)\"\n",
        "FEATURE_COLS = [\n",
        "\"Suction (kPa)\",\n",
        "\"Silica fume (%)\",\n",
        "\"Lime (%)\",\n",
        "\"Gypsum content (%)\",\n",
        "\"Applied vertical stress (kPa)\",\n",
        "\"Degree of Saturation (%)\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "aIIefhsyxqDZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIIefhsyxqDZ",
        "outputId": "eaf62e78-afcd-410d-dd64-248e06a32357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XoxlWiBcxr1B",
      "metadata": {
        "id": "XoxlWiBcxr1B"
      },
      "outputs": [],
      "source": [
        "!mkdir \"/content/drive/MyDrive/NNsGA/FNNs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "USxzWSjQxlTO",
      "metadata": {
        "id": "USxzWSjQxlTO"
      },
      "outputs": [],
      "source": [
        "ARTIFACTS_DIR = \"/content/drive/MyDrive/NNsGA/FNNs/artifacts\"\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tyjyJ3K000Fv",
      "metadata": {
        "id": "tyjyJ3K000Fv"
      },
      "outputs": [],
      "source": [
        "# Utilities\n",
        "def rmse(y_true, y_pred):\n",
        "  return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "def mape(y_true, y_pred, eps=1e-8):\n",
        "  y_true = np.asarray(y_true)\n",
        "  y_pred = np.asarray(y_pred)\n",
        "  return np.mean(np.abs((y_true - y_pred) / (np.clip(np.abs(y_true), eps, None)))) * 100.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "woKjUSXE1Ace",
      "metadata": {
        "id": "woKjUSXE1Ace"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrainConfig:\n",
        "    mfs_per_feature: int = 3 # M\n",
        "    batch_size: int = 128\n",
        "    max_epochs: int = 400\n",
        "    lr: float       = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    patience: int   = 40 # early stopping\n",
        "    warmup_epochs: int = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MSwf5x0q2Eh4",
      "metadata": {
        "id": "MSwf5x0q2Eh4"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "class TabDataset(Dataset):\n",
        "  def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "    self.X = torch.from_numpy(X.astype(np.float32))\n",
        "    self.y = torch.from_numpy(y.astype(np.float32)).view(-1, 1)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nF0AgWg62UI1",
      "metadata": {
        "id": "nF0AgWg62UI1"
      },
      "outputs": [],
      "source": [
        "# Neuro‑Fuzzy (TSK) Model\n",
        "class TSKFuzzyRegressor(nn.Module):\n",
        "    \"\"\"First‑order TSK neuro‑fuzzy network with Gaussian MFs and grid rules.\n",
        "\n",
        "    Input: x in R^D\n",
        "    - For each feature j, we have M Gaussian MFs: mu_{j,m}(x_j) = exp(-0.5 * ((x_j - c_{j,m})/s_{j,m})^2)\n",
        "    - Rules are the Cartesian product of feature MFs → R = M^D rules.\n",
        "    - Firing strength w_r(x) = Π_j mu_{j, m_j}(x_j)\n",
        "    - Consequent per rule r: y_r(x) = a_{r,0} + Σ_j a_{r,j} * x_j\n",
        "    - Output: y(x) = Σ_r [ (w_r / Σ_k w_k) * y_r(x) ]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, D: int, M: int):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.M = M\n",
        "        self.R = M ** D\n",
        "\n",
        "        # MF parameters per feature\n",
        "        # centers: (D, M), sigmas: (D, M) (positivity via softplus)\n",
        "        self.centers = nn.Parameter(torch.zeros(D, M))\n",
        "        self.log_sigmas = nn.Parameter(torch.zeros(D, M))  # sigma = softplus(log_sigma)\n",
        "\n",
        "        # Rule index tensor: (R, D) with values in [0, M-1]\n",
        "        combos = np.stack(np.meshgrid(*[np.arange(M) for _ in range(D)], indexing='ij'), axis=-1).reshape(-1, D)\n",
        "        self.register_buffer('rule_index', torch.from_numpy(combos).long())\n",
        "\n",
        "        # Linear consequents per rule: a0 (bias) + a per feature\n",
        "        self.consequents = nn.Linear(D, self.R, bias=True)  # will output (N, R) of Σ_j a_{r,j} x_j + a_{r,0}\n",
        "\n",
        "        # small epsilon to stabilize normalization\n",
        "        self.eps = 1e-8\n",
        "\n",
        "    def gaussian_mf(self, x):\n",
        "        \"\"\"Compute membership values for all features & MFs.\n",
        "        x: (N, D)\n",
        "        return: mu of shape (N, D, M)\n",
        "        \"\"\"\n",
        "        N, D = x.shape\n",
        "        centers = self.centers  # (D, M)\n",
        "        sigmas = torch.nn.functional.softplus(self.log_sigmas) + 1e-4  # (D, M)\n",
        "        # expand for broadcasting\n",
        "        x_exp = x.unsqueeze(-1)              # (N, D, 1)\n",
        "        c_exp = centers.unsqueeze(0)        # (1, D, M)\n",
        "        s_exp = sigmas.unsqueeze(0)         # (1, D, M)\n",
        "        z = (x_exp - c_exp) / s_exp\n",
        "        mu = torch.exp(-0.5 * z * z)        # (N, D, M)\n",
        "        return mu\n",
        "\n",
        "    def rule_firing(self, mu):\n",
        "        \"\"\"Compute rule firing strengths w_r via product across selected MFs.\n",
        "        mu: (N, D, M)\n",
        "        returns: w of shape (N, R)\n",
        "        \"\"\"\n",
        "        N, D, M = mu.shape\n",
        "        gather_list = []\n",
        "        for j in range(D):\n",
        "            mu_j = mu[:, j, :]                       # (N, M)\n",
        "            mu_jg = mu_j.index_select(dim=1, index=self.rule_index[:, j]).view(N, -1)  # (N, R)\n",
        "            gather_list.append(mu_jg)\n",
        "        w = torch.ones_like(gather_list[0])\n",
        "        for g in gather_list:\n",
        "            w = w * g\n",
        "        return w  # (N, R)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, D)\n",
        "        mu = self.gaussian_mf(x)           # (N, D, M)\n",
        "        w = self.rule_firing(mu)           # (N, R)\n",
        "        w_sum = w.sum(dim=1, keepdim=True) # (N, 1)\n",
        "        beta = w / (w_sum + self.eps)      # normalized firing strengths\n",
        "\n",
        "        # linear consequents per rule for each sample\n",
        "        # consequents(x): (N, R) representing Σ_j a_{r,j} x_j + a_{r,0}\n",
        "        y_lin = self.consequents(x)        # (N, R)\n",
        "        y = (beta * y_lin).sum(dim=1, keepdim=True)  # (N, 1)\n",
        "        return y, w_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40Z2V8j62bBG",
      "metadata": {
        "id": "40Z2V8j62bBG"
      },
      "outputs": [],
      "source": [
        "# Initialization helpers\n",
        "def init_mfs_from_data(model: TSKFuzzyRegressor, X_train: np.ndarray):\n",
        "    \"\"\"Initialize MF centers using feature percentiles and sigmas using spread.\"\"\"\n",
        "    D = X_train.shape[1]\n",
        "    M = model.M\n",
        "    for j in range(D):\n",
        "        # centers from percentiles between 5th..95th\n",
        "        perc = np.linspace(5, 95, M)\n",
        "        c = np.percentile(X_train[:, j], perc)\n",
        "        # ensure sorted and unique-ish\n",
        "        c = np.unique(np.round(c, 6))\n",
        "        if c.size < M:\n",
        "            # pad by small jitter around median\n",
        "            med = np.median(X_train[:, j])\n",
        "            pad = np.linspace(-1, 1, M - c.size) * np.std(X_train[:, j]) * 0.1 + med\n",
        "            c = np.sort(np.concatenate([c, pad]))\n",
        "        s = np.full(M, np.std(X_train[:, j]) + 1e-3)\n",
        "        with torch.no_grad():\n",
        "            model.centers[j].copy_(torch.from_numpy(c.astype(np.float32)))\n",
        "            model.log_sigmas[j].copy_(torch.log(torch.from_numpy(s.astype(np.float32))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e650LN322jtu",
      "metadata": {
        "id": "e650LN322jtu"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, cfg: TrainConfig):\n",
        "    model.to(DEVICE)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, cfg.max_epochs - cfg.warmup_epochs))\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    history = {\"train\": [], \"val\": [], \"lr\": []}\n",
        "    patience = cfg.patience\n",
        "\n",
        "    for epoch in range(1, cfg.max_epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            pred, _ = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        train_loss = float(np.mean(train_losses))\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(DEVICE)\n",
        "                yb = yb.to(DEVICE)\n",
        "                pred, _ = model(xb)\n",
        "                loss = criterion(pred, yb)\n",
        "                val_losses.append(loss.item())\n",
        "        val_loss = float(np.mean(val_losses))\n",
        "\n",
        "        # LR scheduling (simple: step after warmup period)\n",
        "        if epoch > cfg.warmup_epochs:\n",
        "            scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        history[\"train\"].append(train_loss)\n",
        "        history[\"val\"].append(val_loss)\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "        # early stopping\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience = cfg.patience\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience <= 0:\n",
        "                break\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:04d} | train MSE={train_loss:.4f} | val MSE={val_loss:.4f} | lr={current_lr:.2e}\")\n",
        "\n",
        "    # restore best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jsKGN1gW2pL-",
      "metadata": {
        "id": "jsKGN1gW2pL-"
      },
      "outputs": [],
      "source": [
        "# Evaluation helpers\n",
        "def evaluate(model, X: np.ndarray, y: np.ndarray) -> Tuple[dict, np.ndarray]:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_t = torch.from_numpy(X.astype(np.float32)).to(DEVICE)\n",
        "        y_hat, _ = model(X_t)\n",
        "        y_hat = y_hat.cpu().numpy().reshape(-1)\n",
        "    metrics = {\n",
        "        \"RMSE\": rmse(y, y_hat),\n",
        "        \"MAE\": mean_absolute_error(y, y_hat),\n",
        "        \"R2\": r2_score(y, y_hat),\n",
        "        \"MAPE_%\": mape(y, y_hat),\n",
        "    }\n",
        "    return metrics, y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uu9LwXe72vjG",
      "metadata": {
        "id": "uu9LwXe72vjG"
      },
      "outputs": [],
      "source": [
        "def plot_training(history: dict, outdir: str):\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"train\"], label=\"Train MSE\")\n",
        "    plt.plot(history[\"val\"], label=\"Val MSE\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training/Validation Loss\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, \"loss_curves.png\"), dpi=160)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_4sgRp-B206u",
      "metadata": {
        "id": "_4sgRp-B206u"
      },
      "outputs": [],
      "source": [
        "def plot_parity(y_true: np.ndarray, y_pred: np.ndarray, outdir: str, split_name: str):\n",
        "    plt.figure()\n",
        "    plt.scatter(y_true, y_pred, s=14, alpha=0.7)\n",
        "    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
        "    plt.plot(lims, lims)\n",
        "    plt.xlabel(\"Actual Collapse Potential (%)\")\n",
        "    plt.ylabel(\"Predicted Collapse Potential (%)\")\n",
        "    plt.title(f\"Parity Plot — {split_name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"parity_{split_name.lower()}.png\"), dpi=160)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U7Rdh1Wj23w2",
      "metadata": {
        "id": "U7Rdh1Wj23w2"
      },
      "outputs": [],
      "source": [
        "def plot_residuals(y_true: np.ndarray, y_pred: np.ndarray, outdir: str, split_name: str):\n",
        "    res = y_pred - y_true\n",
        "    plt.figure()\n",
        "    plt.hist(res, bins=40)\n",
        "    plt.xlabel(\"Residual (Pred − True)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(f\"Residuals — {split_name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"residuals_{split_name.lower()}.png\"), dpi=160)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NdAOhGr624tI",
      "metadata": {
        "id": "NdAOhGr624tI"
      },
      "outputs": [],
      "source": [
        "def permutation_feature_importance(model, X_val, y_val, scaler: StandardScaler, n_repeats: int = 8):\n",
        "    # simple, model-agnostic permutation importance\n",
        "    base_metrics, base_pred = evaluate(model, X_val, y_val)\n",
        "    base_rmse = base_metrics[\"RMSE\"]\n",
        "    D = X_val.shape[1]\n",
        "    importances = np.zeros(D)\n",
        "    for j in range(D):\n",
        "        worsens = []\n",
        "        for _ in range(n_repeats):\n",
        "            Xp = X_val.copy()\n",
        "            np.random.shuffle(Xp[:, j])\n",
        "            m, _ = evaluate(model, Xp, y_val)\n",
        "            worsens.append(m[\"RMSE\"] - base_rmse)\n",
        "        importances[j] = np.mean(worsens)\n",
        "    return importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fNljI_o_27eS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNljI_o_27eS",
        "outputId": "edef0e8b-c1a6-42ac-a9aa-1123d4f51bc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (600, 7)\n",
            "       Suction (kPa)  Silica fume (%)    Lime (%)  Gypsum content (%)  \\\n",
            "count     600.000000       600.000000  600.000000          600.000000   \n",
            "mean     2542.050000         2.000000    2.000000           40.000000   \n",
            "std      7112.482625         2.830787    2.830787           20.141403   \n",
            "min         5.000000         0.000000    0.000000           15.000000   \n",
            "25%        45.500000         0.000000    0.000000           15.000000   \n",
            "50%       205.000000         0.000000    0.000000           35.000000   \n",
            "75%       850.000000         4.000000    4.000000           60.000000   \n",
            "max     32000.000000         8.000000    8.000000           70.000000   \n",
            "\n",
            "       Applied vertical stress (kPa)  Degree of Saturation (%)  \\\n",
            "count                     600.000000                600.000000   \n",
            "mean                      182.916667                 44.937667   \n",
            "std                        51.529541                 30.747022   \n",
            "min                        12.500000                 10.000000   \n",
            "25%                       200.000000                 15.000000   \n",
            "50%                       200.000000                 40.000000   \n",
            "75%                       200.000000                 75.000000   \n",
            "max                       200.000000                119.000000   \n",
            "\n",
            "       Collapse Potential (%)  \n",
            "count              600.000000  \n",
            "mean                 5.755467  \n",
            "std                 13.691497  \n",
            "min                  0.005000  \n",
            "25%                  0.500000  \n",
            "50%                  2.000000  \n",
            "75%                  4.700000  \n",
            "max                 80.000000  \n",
            "Epoch 0001 | train MSE=180.0124 | val MSE=145.9152 | lr=1.00e-03\n",
            "Epoch 0010 | train MSE=214.5997 | val MSE=143.7669 | lr=1.00e-03\n",
            "Epoch 0020 | train MSE=199.7785 | val MSE=141.3942 | lr=9.98e-04\n",
            "Epoch 0030 | train MSE=221.5879 | val MSE=139.0883 | lr=9.94e-04\n",
            "Epoch 0040 | train MSE=255.4441 | val MSE=136.7864 | lr=9.85e-04\n",
            "Epoch 0050 | train MSE=188.6754 | val MSE=134.4003 | lr=9.74e-04\n",
            "Epoch 0060 | train MSE=226.4222 | val MSE=131.8915 | lr=9.60e-04\n",
            "Epoch 0070 | train MSE=164.9924 | val MSE=129.3038 | lr=9.43e-04\n",
            "Epoch 0080 | train MSE=227.3092 | val MSE=126.5071 | lr=9.23e-04\n",
            "Epoch 0090 | train MSE=162.5532 | val MSE=123.2911 | lr=9.00e-04\n",
            "Epoch 0100 | train MSE=185.9798 | val MSE=119.7619 | lr=8.74e-04\n",
            "Epoch 0110 | train MSE=201.0373 | val MSE=116.2465 | lr=8.46e-04\n",
            "Epoch 0120 | train MSE=173.1136 | val MSE=112.5823 | lr=8.16e-04\n",
            "Epoch 0130 | train MSE=144.5686 | val MSE=108.9210 | lr=7.84e-04\n",
            "Epoch 0140 | train MSE=176.5343 | val MSE=105.5038 | lr=7.50e-04\n",
            "Epoch 0150 | train MSE=173.0219 | val MSE=102.4237 | lr=7.14e-04\n",
            "Epoch 0160 | train MSE=162.8872 | val MSE=99.6707 | lr=6.77e-04\n",
            "Epoch 0170 | train MSE=155.3860 | val MSE=97.2468 | lr=6.39e-04\n",
            "Epoch 0180 | train MSE=158.9828 | val MSE=95.1177 | lr=6.00e-04\n",
            "Epoch 0190 | train MSE=133.7961 | val MSE=93.3046 | lr=5.60e-04\n",
            "Epoch 0200 | train MSE=160.4337 | val MSE=91.7603 | lr=5.20e-04\n",
            "Epoch 0210 | train MSE=150.3557 | val MSE=90.3799 | lr=4.80e-04\n",
            "Epoch 0220 | train MSE=141.5953 | val MSE=89.1772 | lr=4.40e-04\n",
            "Epoch 0230 | train MSE=127.2182 | val MSE=88.1332 | lr=4.00e-04\n",
            "Epoch 0240 | train MSE=200.9601 | val MSE=87.2618 | lr=3.61e-04\n",
            "Epoch 0250 | train MSE=122.2856 | val MSE=86.4689 | lr=3.23e-04\n",
            "Epoch 0260 | train MSE=169.0606 | val MSE=85.8162 | lr=2.86e-04\n",
            "Epoch 0270 | train MSE=120.9556 | val MSE=85.2635 | lr=2.50e-04\n",
            "Epoch 0280 | train MSE=134.1357 | val MSE=84.7934 | lr=2.16e-04\n",
            "Epoch 0290 | train MSE=171.0389 | val MSE=84.3837 | lr=1.84e-04\n",
            "Epoch 0300 | train MSE=179.4184 | val MSE=84.0509 | lr=1.54e-04\n",
            "Epoch 0310 | train MSE=119.5117 | val MSE=83.7695 | lr=1.26e-04\n",
            "Epoch 0320 | train MSE=119.5867 | val MSE=83.5718 | lr=1.00e-04\n",
            "Epoch 0330 | train MSE=128.5603 | val MSE=83.4176 | lr=7.74e-05\n",
            "Epoch 0340 | train MSE=141.6804 | val MSE=83.2933 | lr=5.73e-05\n",
            "Epoch 0350 | train MSE=178.2550 | val MSE=83.2081 | lr=4.00e-05\n",
            "Epoch 0360 | train MSE=141.8273 | val MSE=83.1503 | lr=2.57e-05\n",
            "Epoch 0370 | train MSE=118.5543 | val MSE=83.1146 | lr=1.45e-05\n",
            "Epoch 0380 | train MSE=182.0771 | val MSE=83.0958 | lr=6.47e-06\n",
            "Epoch 0390 | train MSE=144.9584 | val MSE=83.0886 | lr=1.62e-06\n",
            "Epoch 0400 | train MSE=166.7561 | val MSE=83.0876 | lr=0.00e+00\n",
            "Training finished in 0.1 min. Best val MSE: 83.0876\n",
            "\n",
            "Metrics (Train) {\n",
            "  \"RMSE\": 11.999285358765903,\n",
            "  \"MAE\": 3.853579521179199,\n",
            "  \"R2\": 0.22180140018463135,\n",
            "  \"MAPE_%\": 689.106201171875\n",
            "}\n",
            "Metrics (Val)   {\n",
            "  \"RMSE\": 9.115237932222897,\n",
            "  \"MAE\": 3.1535165309906006,\n",
            "  \"R2\": 0.3246345520019531,\n",
            "  \"MAPE_%\": 1161.87890625\n",
            "}\n",
            "Metrics (Test)  {\n",
            "  \"RMSE\": 14.356740483635313,\n",
            "  \"MAE\": 5.1571364402771,\n",
            "  \"R2\": 0.1960086226463318,\n",
            "  \"MAPE_%\": 389.1484375\n",
            "}\n",
            "\n",
            "Computing permutation feature importance on validation split…\n",
            "Permutation importance saved to: /content/drive/MyDrive/NNsGA/FNNs/artifacts/permutation_importance.csv\n",
            "Predictions saved to: /content/drive/MyDrive/NNsGA/FNNs/artifacts/predictions.csv\n",
            "\n",
            "Summary saved to: /content/drive/MyDrive/NNsGA/FNNs/artifacts/summary.json\n"
          ]
        }
      ],
      "source": [
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = TrainConfig()\n",
        "\n",
        "    # Load data\n",
        "    if not os.path.exists(DATA_CSV):\n",
        "        raise FileNotFoundError(\n",
        "            f\"File not found at {DATA_CSV}. Please set DATA_CSV to your file path.\\n\"\n",
        "            \"Expected columns: ['Suction_kPa','SilicaFume_pct','Lime_pct','Gypsum_pct',\\n\"\n",
        "            \" 'AppliedVerticalStress_kPa','DegreeSaturation_pct','CollapsePotential_pct']\"\n",
        "        )\n",
        "\n",
        "    df = pd.read_excel(DATA_CSV)\n",
        "    for col in FEATURE_COLS + [TARGET_COL]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Missing column: {col}\")\n",
        "\n",
        "    # basic sanity prints\n",
        "    print(\"Data shape:\", df.shape)\n",
        "    print(df[FEATURE_COLS + [TARGET_COL]].describe())\n",
        "\n",
        "    X = df[FEATURE_COLS].values.astype(np.float32)\n",
        "    y = df[TARGET_COL].values.astype(np.float32)\n",
        "\n",
        "    # Split 70/15/15\n",
        "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=SEED)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=SEED)\n",
        "\n",
        "    #  Scale features (fit only on train)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_val_s = scaler.transform(X_val)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    #  Build model\n",
        "    D = X_train_s.shape[1]\n",
        "    M = cfg.mfs_per_feature\n",
        "    model = TSKFuzzyRegressor(D=D, M=M)\n",
        "    init_mfs_from_data(model, X_train_s)\n",
        "\n",
        "    #  Dataloaders\n",
        "    train_ds = TabDataset(X_train_s, y_train)\n",
        "    val_ds   = TabDataset(X_val_s, y_val)\n",
        "    test_ds  = TabDataset(X_test_s, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    # Train\n",
        "    start   = time.time()\n",
        "    history = train_model(model, train_loader, val_loader, cfg)\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Training finished in {elapsed/60:.1f} min. Best val MSE: {min(history['val']):.4f}\")\n",
        "\n",
        "    # Save artifacts\n",
        "    model_path = os.path.join(ARTIFACTS_DIR, \"tsk_fuzzy_regressor.pt\")\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"config\": cfg.__dict__,\n",
        "        \"feature_cols\": FEATURE_COLS,\n",
        "        \"target_col\": TARGET_COL,\n",
        "        \"scaler_mean_\": scaler.mean_.tolist(),\n",
        "        \"scaler_scale_\": scaler.scale_.tolist(),\n",
        "    }, model_path)\n",
        "    with open(os.path.join(ARTIFACTS_DIR, \"scaler.json\"), \"w\") as f:\n",
        "        json.dump({\"mean\": scaler.mean_.tolist(), \"scale\": scaler.scale_.tolist()}, f, indent=2)\n",
        "\n",
        "    # Evaluate\n",
        "    train_metrics, yhat_train = evaluate(model, X_train_s, y_train)\n",
        "    val_metrics, yhat_val = evaluate(model, X_val_s, y_val)\n",
        "    test_metrics, yhat_test = evaluate(model, X_test_s, y_test)\n",
        "\n",
        "    # Convert float32 values to standard floats for JSON serialization\n",
        "    train_metrics_json = {k: float(v) for k, v in train_metrics.items()}\n",
        "    val_metrics_json  = {k: float(v) for k, v in val_metrics.items()}\n",
        "    test_metrics_json = {k: float(v) for k, v in test_metrics.items()}\n",
        "\n",
        "    print(\"\\nMetrics (Train)\", json.dumps(train_metrics_json, indent=2))\n",
        "    print(\"Metrics (Val)  \", json.dumps(val_metrics_json, indent=2))\n",
        "    print(\"Metrics (Test) \", json.dumps(test_metrics_json, indent=2))\n",
        "\n",
        "    # Plots\n",
        "    plot_training(history, ARTIFACTS_DIR)\n",
        "    plot_parity(y_train, yhat_train, ARTIFACTS_DIR, \"Train\")\n",
        "    plot_parity(y_val, yhat_val, ARTIFACTS_DIR, \"Val\")\n",
        "    plot_parity(y_test, yhat_test, ARTIFACTS_DIR, \"Test\")\n",
        "    plot_residuals(y_train, yhat_train, ARTIFACTS_DIR, \"Train\")\n",
        "    plot_residuals(y_val, yhat_val, ARTIFACTS_DIR, \"Val\")\n",
        "    plot_residuals(y_test, yhat_test, ARTIFACTS_DIR, \"Test\")\n",
        "\n",
        "    # Permutation importance (on validation split)\n",
        "    print(\"\\nComputing permutation feature importance on validation split…\")\n",
        "    importances = permutation_feature_importance(model, X_val_s, y_val, scaler, n_repeats=5)\n",
        "    imp_df = pd.DataFrame({\"feature\": FEATURE_COLS, \"importance_RMSE_worsening\": importances})\n",
        "    imp_df = imp_df.sort_values(\"importance_RMSE_worsening\", ascending=False)\n",
        "    imp_path = os.path.join(ARTIFACTS_DIR, \"permutation_importance.csv\")\n",
        "    imp_df.to_csv(imp_path, index=False)\n",
        "    print(\"Permutation importance saved to:\", imp_path)\n",
        "\n",
        "    # quick bar plot\n",
        "    plt.figure()\n",
        "    plt.barh(imp_df[\"feature\"], imp_df[\"importance_RMSE_worsening\"])  # no custom colors\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.xlabel(\"ΔRMSE after permutation (Validation)\")\n",
        "    plt.title(\"Permutation Feature Importance\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(ARTIFACTS_DIR, \"perm_importance.png\"), dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    # Save predictions\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"split\": [\"train\"] * len(y_train) + [\"val\"] * len(y_val) + [\"test\"] * len(y_test),\n",
        "        \"y_true\": np.concatenate([y_train, y_val, y_test]),\n",
        "        \"y_pred\": np.concatenate([yhat_train, yhat_val, yhat_test]),\n",
        "    })\n",
        "    pred_path = os.path.join(ARTIFACTS_DIR, \"predictions.csv\")\n",
        "    pred_df.to_csv(pred_path, index=False)\n",
        "    print(\"Predictions saved to:\", pred_path)\n",
        "\n",
        "    # Final summary\n",
        "    summary = {\n",
        "        \"train\": train_metrics_json,\n",
        "        \"val\": val_metrics_json,\n",
        "        \"test\": test_metrics_json,\n",
        "        \"artifacts\": {\n",
        "            \"model\": model_path,\n",
        "            \"loss_curves\": os.path.join(ARTIFACTS_DIR, \"loss_curves.png\"),\n",
        "            \"parity_train\": os.path.join(ARTIFACTS_DIR, \"parity_train.png\"),\n",
        "            \"parity_val\": os.path.join(ARTIFACTS_DIR, \"parity_val.png\"),\n",
        "            \"parity_test\": os.path.join(ARTIFACTS_DIR, \"parity_test.png\"),\n",
        "            \"residuals_train\": os.path.join(ARTIFACTS_DIR, \"residuals_train.png\"),\n",
        "            \"residuals_val\": os.path.join(ARTIFACTS_DIR, \"residuals_val.png\"),\n",
        "            \"residuals_test\": os.path.join(ARTIFACTS_DIR, \"residuals_test.png\"),\n",
        "            \"perm_importance\": os.path.join(ARTIFACTS_DIR, \"perm_importance.png\"),\n",
        "            \"perm_importance_csv\": imp_path,\n",
        "            \"predictions_csv\": pred_path,\n",
        "        }\n",
        "    }\n",
        "    with open(os.path.join(ARTIFACTS_DIR, \"summary.json\"), \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(\"\\nSummary saved to:\", os.path.join(ARTIFACTS_DIR, \"summary.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lsqMaGER89xr",
      "metadata": {
        "id": "lsqMaGER89xr"
      },
      "source": [
        "## Second Expierment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1b8ec3eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b8ec3eb",
        "outputId": "0c0a6105-0623-4038-c692-923974e43801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "%pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0wDbYfY6KkG_",
      "metadata": {
        "id": "0wDbYfY6KkG_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WgB1av2BSk6g",
      "metadata": {
        "id": "WgB1av2BSk6g"
      },
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Config\n",
        "DATA_CSV = \"/content/drive/MyDrive/PINNs/Suction_vsCP-modified_1.xlsx\"\n",
        "TARGET_COL = \"Collapse Potential (%)\"\n",
        "FEATURE_COLS = [\n",
        "    \"Suction (kPa)\",\n",
        "    \"Silica fume (%)\",\n",
        "    \"Lime (%)\",\n",
        "    \"Gypsum content (%)\",\n",
        "    \"Applied vertical stress (kPa)\",\n",
        "    \"Degree of Saturation (%)\",\n",
        "]\n",
        "\n",
        "ARTIFACTS_DIR = \"/content/drive/MyDrive/NNsGA/FNNs/artifacts_enhanced\"\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7S77PqLvStQh",
      "metadata": {
        "id": "7S77PqLvStQh"
      },
      "outputs": [],
      "source": [
        "# Utilities\n",
        "def rmse(y_true, y_pred):\n",
        "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def mape(y_true, y_pred, eps=1e-8):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / (np.clip(np.abs(y_true), eps, None)))) * 100.0\n",
        "\n",
        "@dataclass\n",
        "class EnhancedTrainConfig:\n",
        "    mfs_per_feature: int = 2  # Reduced to prevent rule explosion (2^6=64 rules)\n",
        "    batch_size: int = 64\n",
        "    max_epochs: int = 500\n",
        "    lr: float = 5e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    patience: int = 50\n",
        "    warmup_epochs: int = 15\n",
        "    dropout_rate: float = 0.2\n",
        "    rule_dropout_rate: float = 0.1\n",
        "    use_feature_attention: bool = True\n",
        "    use_rule_importance: bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k5T_9SswSwzY",
      "metadata": {
        "id": "k5T_9SswSwzY"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "class TabDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y.astype(np.float32)).view(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arEcJCKnS1Re",
      "metadata": {
        "id": "arEcJCKnS1Re"
      },
      "outputs": [],
      "source": [
        "# Enhanced TSK Model\n",
        "class EnhancedTSKFuzzyRegressor(nn.Module):\n",
        "    def __init__(self, D: int, M: int, config: EnhancedTrainConfig):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.M = M\n",
        "        self.config = config\n",
        "        self.R = M ** D\n",
        "\n",
        "        # MF parameters\n",
        "        self.centers = nn.Parameter(torch.zeros(D, M))\n",
        "        self.log_sigmas = nn.Parameter(torch.zeros(D, M))\n",
        "\n",
        "        # Rule index tensor\n",
        "        combos = np.stack(np.meshgrid(*[np.arange(M) for _ in range(D)], indexing='ij'), axis=-1).reshape(-1, D)\n",
        "        self.register_buffer('rule_index', torch.from_numpy(combos).long())\n",
        "\n",
        "        # Enhanced components\n",
        "        if config.use_rule_importance:\n",
        "            self.rule_importance = nn.Parameter(torch.ones(self.R))\n",
        "\n",
        "        if config.use_feature_attention:\n",
        "            self.feature_attention = nn.Linear(D, D)\n",
        "\n",
        "        # Consequents\n",
        "        self.consequents = nn.Linear(D, self.R, bias=True)\n",
        "\n",
        "        # Regularization\n",
        "        self.dropout = nn.Dropout(config.dropout_rate)\n",
        "        self.rule_dropout = nn.Dropout(config.rule_dropout_rate)\n",
        "\n",
        "        self.eps = 1e-8\n",
        "\n",
        "    def gaussian_mf(self, x):\n",
        "        N, D = x.shape\n",
        "        centers = self.centers\n",
        "        sigmas = torch.nn.functional.softplus(self.log_sigmas) + 1e-4\n",
        "\n",
        "        # Apply feature attention if enabled\n",
        "        if self.config.use_feature_attention:\n",
        "            x_att = torch.sigmoid(self.feature_attention(x)) * x\n",
        "        else:\n",
        "            x_att = x\n",
        "\n",
        "        x_exp = x_att.unsqueeze(-1)\n",
        "        c_exp = centers.unsqueeze(0)\n",
        "        s_exp = sigmas.unsqueeze(0)\n",
        "\n",
        "        z = (x_exp - c_exp) / s_exp\n",
        "        mu = torch.exp(-0.5 * z * z)\n",
        "        return mu\n",
        "\n",
        "    def rule_firing(self, mu):\n",
        "        N, D, M = mu.shape\n",
        "        gather_list = []\n",
        "        for j in range(D):\n",
        "            mu_j = mu[:, j, :]\n",
        "            mu_jg = mu_j.index_select(dim=1, index=self.rule_index[:, j]).view(N, -1)\n",
        "            gather_list.append(mu_jg)\n",
        "        w = torch.ones_like(gather_list[0])\n",
        "        for g in gather_list:\n",
        "            w = w * g\n",
        "        return w\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = self.gaussian_mf(x)\n",
        "        w = self.rule_firing(mu)\n",
        "\n",
        "        # Apply rule importance if enabled\n",
        "        if self.config.use_rule_importance:\n",
        "            w = w * torch.sigmoid(self.rule_importance).unsqueeze(0)\n",
        "\n",
        "        # Apply rule dropout\n",
        "        w = self.rule_dropout(w)\n",
        "\n",
        "        w_sum = w.sum(dim=1, keepdim=True)\n",
        "        beta = w / (w_sum + self.eps)\n",
        "\n",
        "        # Apply dropout to input for consequents\n",
        "        x_drop = self.dropout(x)\n",
        "        y_lin = self.consequents(x_drop)\n",
        "\n",
        "        y = (beta * y_lin).sum(dim=1, keepdim=True)\n",
        "        return y, w_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nr_I9dzkS6Fm",
      "metadata": {
        "id": "Nr_I9dzkS6Fm"
      },
      "outputs": [],
      "source": [
        "# Enhanced Initialization\n",
        "def enhanced_init_mfs(model, X_train):\n",
        "    D = X_train.shape[1]\n",
        "    M = model.M\n",
        "\n",
        "    for j in range(D):\n",
        "        # Use K-means for better center initialization\n",
        "        kmeans = KMeans(n_clusters=M, random_state=SEED+j)\n",
        "        kmeans.fit(X_train[:, j].reshape(-1, 1))\n",
        "        centers = np.sort(kmeans.cluster_centers_.flatten())\n",
        "\n",
        "        # Ensure we have exactly M centers\n",
        "        if len(centers) < M:\n",
        "            # Pad with evenly spaced values\n",
        "            min_val, max_val = np.min(X_train[:, j]), np.max(X_train[:, j])\n",
        "            additional_centers = np.linspace(min_val, max_val, M - len(centers) + 2)[1:-1]\n",
        "            centers = np.sort(np.concatenate([centers, additional_centers]))\n",
        "\n",
        "        # Adaptive sigma based on cluster spread\n",
        "        sigma_base = np.std(X_train[:, j]) * 0.5\n",
        "        sigmas = np.full(M, sigma_base)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.centers[j].copy_(torch.from_numpy(centers.astype(np.float32)))\n",
        "            model.log_sigmas[j].copy_(torch.log(torch.from_numpy(sigmas.astype(np.float32))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vuIpRhDlTAuf",
      "metadata": {
        "id": "vuIpRhDlTAuf"
      },
      "outputs": [],
      "source": [
        "# Enhanced Training\n",
        "def validate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            pred, _ = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            val_losses.append(loss.item())\n",
        "    return float(np.mean(val_losses))\n",
        "\n",
        "def enhanced_train_model(model, train_loader, val_loader, cfg: EnhancedTrainConfig):\n",
        "    model.to(DEVICE)\n",
        "    criterion = nn.HuberLoss()  # More robust than MSE\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    history = {\"train\": [], \"val\": [], \"lr\": []}\n",
        "    patience = cfg.patience\n",
        "\n",
        "    for epoch in range(1, cfg.max_epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            pred, _ = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "\n",
        "            # Add L1 regularization for consequents\n",
        "            l1_reg = torch.tensor(0.).to(DEVICE)\n",
        "            for param in model.consequents.parameters():\n",
        "                l1_reg += torch.norm(param, 1)\n",
        "            loss += 1e-4 * l1_reg\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = float(np.mean(train_losses))\n",
        "        val_loss = validate_model(model, val_loader, criterion)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        history[\"train\"].append(train_loss)\n",
        "        history[\"val\"].append(val_loss)\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience = cfg.patience\n",
        "        else:\n",
        "            patience -= 1\n",
        "\n",
        "        if epoch % 20 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:04d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.2e}\")\n",
        "\n",
        "        if patience <= 0:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1jnuUB-ITEv7",
      "metadata": {
        "id": "1jnuUB-ITEv7"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Optimization\n",
        "def optimize_hyperparameters(X_train, y_train, X_val, y_val):\n",
        "    def objective(trial):\n",
        "        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "        dropout = trial.suggest_float('dropout', 0.1, 0.4)\n",
        "        mfs = trial.suggest_int('mfs_per_feature', 2, 3)\n",
        "\n",
        "        config = EnhancedTrainConfig(\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            dropout_rate=dropout,\n",
        "            mfs_per_feature=mfs\n",
        "        )\n",
        "\n",
        "        model = EnhancedTSKFuzzyRegressor(X_train.shape[1], config.mfs_per_feature, config)\n",
        "        enhanced_init_mfs(model, X_train)\n",
        "\n",
        "        # Create data loaders for optimization\n",
        "        train_ds = TabDataset(X_train, y_train)\n",
        "        val_ds = TabDataset(X_val, y_val)\n",
        "        train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "        # Quick training\n",
        "        model.to(DEVICE)\n",
        "        criterion = nn.HuberLoss()\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "\n",
        "        model.train()\n",
        "        for _ in range(30):  # Short training for optimization\n",
        "            for xb, yb in train_loader:\n",
        "                xb = xb.to(DEVICE)\n",
        "                yb = yb.to(DEVICE)\n",
        "                pred, _ = model(xb)\n",
        "                loss = criterion(pred, yb)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Validation score\n",
        "        val_loss = validate_model(model, val_loader, criterion)\n",
        "        return val_loss\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=15, timeout=3600)  # 1 hour timeout\n",
        "\n",
        "    print(\"Best hyperparameters:\", study.best_params)\n",
        "    return study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4hX8QVkUTIkf",
      "metadata": {
        "id": "4hX8QVkUTIkf"
      },
      "outputs": [],
      "source": [
        "# Evaluation and Plotting (unchanged from original)\n",
        "def evaluate(model, X: np.ndarray, y: np.ndarray) -> Tuple[dict, np.ndarray]:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_t = torch.from_numpy(X.astype(np.float32)).to(DEVICE)\n",
        "        y_hat, _ = model(X_t)\n",
        "        y_hat = y_hat.cpu().numpy().reshape(-1)\n",
        "    metrics = {\n",
        "        \"RMSE\": rmse(y, y_hat),\n",
        "        \"MAE\": mean_absolute_error(y, y_hat),\n",
        "        \"R2\": r2_score(y, y_hat),\n",
        "        \"MAPE_%\": mape(y, y_hat),\n",
        "    }\n",
        "    return metrics, y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "3j5K9MonTLkY",
      "metadata": {
        "id": "3j5K9MonTLkY"
      },
      "outputs": [],
      "source": [
        "def plot_training(history: dict, outdir: str):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history[\"train\"], label=\"Train Loss\", alpha=0.8)\n",
        "    plt.plot(history[\"val\"], label=\"Validation Loss\", alpha=0.8)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, \"loss_curves.png\"), dpi=160)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "gb08NROWTMTe",
      "metadata": {
        "id": "gb08NROWTMTe"
      },
      "outputs": [],
      "source": [
        "def plot_parity(y_true: np.ndarray, y_pred: np.ndarray, outdir: str, split_name: str):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_true, y_pred, s=30, alpha=0.6, edgecolors='w', linewidth=0.5)\n",
        "    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
        "    plt.plot(lims, lims, 'r--', alpha=0.8)\n",
        "    plt.xlabel(\"Actual Collapse Potential (%)\")\n",
        "    plt.ylabel(\"Predicted Collapse Potential (%)\")\n",
        "    plt.title(f\"Parity Plot — {split_name}\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"parity_{split_name.lower()}.png\"), dpi=160)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WUnUcSKtTPrs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUnUcSKtTPrs",
        "outputId": "e8c87cec-f58f-46ea-909c-e13c703f2f29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-20 07:07:03,033] A new study created in memory with name: no-name-cb083801-732a-4cb5-934a-5ca39e4aab7b\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Data shape: (600, 7)\n",
            "Features: ['Suction (kPa)', 'Silica fume (%)', 'Lime (%)', 'Gypsum content (%)', 'Applied vertical stress (kPa)', 'Degree of Saturation (%)']\n",
            "Training set: (420, 6), Validation set: (90, 6), Test set: (90, 6)\n",
            "Optimizing hyperparameters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-20 07:07:05,323] Trial 0 finished with value: 3.138617366552353 and parameters: {'lr': 0.0004706223175404808, 'weight_decay': 1.5757905968828446e-05, 'dropout': 0.12185327001883688, 'mfs_per_feature': 3}. Best is trial 0 with value: 3.138617366552353.\n",
            "[I 2025-08-20 07:07:06,130] Trial 1 finished with value: 3.5144383907318115 and parameters: {'lr': 1.0482499014279655e-05, 'weight_decay': 9.568032318116537e-06, 'dropout': 0.311896610915418, 'mfs_per_feature': 3}. Best is trial 0 with value: 3.138617366552353.\n",
            "[I 2025-08-20 07:07:06,942] Trial 2 finished with value: 3.172644078731537 and parameters: {'lr': 0.0005886232100781116, 'weight_decay': 1.5035300191228642e-05, 'dropout': 0.2864817982874854, 'mfs_per_feature': 3}. Best is trial 0 with value: 3.138617366552353.\n",
            "[I 2025-08-20 07:07:07,755] Trial 3 finished with value: 3.4943421483039856 and parameters: {'lr': 3.2515313889328696e-05, 'weight_decay': 9.336708105205606e-05, 'dropout': 0.389436836264066, 'mfs_per_feature': 3}. Best is trial 0 with value: 3.138617366552353.\n",
            "[I 2025-08-20 07:07:08,587] Trial 4 finished with value: 1.8949120417237282 and parameters: {'lr': 0.00403782223464093, 'weight_decay': 9.30464100797494e-05, 'dropout': 0.31209475778474877, 'mfs_per_feature': 3}. Best is trial 4 with value: 1.8949120417237282.\n",
            "[I 2025-08-20 07:07:09,395] Trial 5 finished with value: 2.6309299170970917 and parameters: {'lr': 0.0015004834639067866, 'weight_decay': 2.480400736697934e-05, 'dropout': 0.1506022951140571, 'mfs_per_feature': 3}. Best is trial 4 with value: 1.8949120417237282.\n",
            "[I 2025-08-20 07:07:10,209] Trial 6 finished with value: 3.565252959728241 and parameters: {'lr': 1.0920626189818818e-05, 'weight_decay': 0.00010002636434400922, 'dropout': 0.3614790721456378, 'mfs_per_feature': 2}. Best is trial 4 with value: 1.8949120417237282.\n",
            "[I 2025-08-20 07:07:11,036] Trial 7 finished with value: 2.9574603140354156 and parameters: {'lr': 0.0007534134321849459, 'weight_decay': 4.06038114013659e-05, 'dropout': 0.2556859788656727, 'mfs_per_feature': 3}. Best is trial 4 with value: 1.8949120417237282.\n",
            "[I 2025-08-20 07:07:11,863] Trial 8 finished with value: 3.2805774211883545 and parameters: {'lr': 0.00029098946707450024, 'weight_decay': 0.0005042296180409844, 'dropout': 0.1038680123618736, 'mfs_per_feature': 2}. Best is trial 4 with value: 1.8949120417237282.\n",
            "[I 2025-08-20 07:07:12,904] Trial 9 finished with value: 3.260438233613968 and parameters: {'lr': 0.00016328272110074517, 'weight_decay': 0.00040287904432774203, 'dropout': 0.18919117215140333, 'mfs_per_feature': 3}. Best is trial 4 with value: 1.8949120417237282.\n",
            "[I 2025-08-20 07:07:13,950] Trial 10 finished with value: 1.6576006561517715 and parameters: {'lr': 0.009204054633270552, 'weight_decay': 1.0127424356258571e-06, 'dropout': 0.20098102919938235, 'mfs_per_feature': 2}. Best is trial 10 with value: 1.6576006561517715.\n",
            "[I 2025-08-20 07:07:15,076] Trial 11 finished with value: 1.881896659731865 and parameters: {'lr': 0.008287197543195238, 'weight_decay': 1.1273132441203358e-06, 'dropout': 0.2122189023404275, 'mfs_per_feature': 2}. Best is trial 10 with value: 1.6576006561517715.\n",
            "[I 2025-08-20 07:07:15,893] Trial 12 finished with value: 1.6551732942461967 and parameters: {'lr': 0.009917188786304152, 'weight_decay': 1.3647101255572975e-06, 'dropout': 0.19891819110613485, 'mfs_per_feature': 2}. Best is trial 12 with value: 1.6551732942461967.\n",
            "[I 2025-08-20 07:07:16,692] Trial 13 finished with value: 1.498906284570694 and parameters: {'lr': 0.009441711029875932, 'weight_decay': 1.0215468948926014e-06, 'dropout': 0.19981367458227509, 'mfs_per_feature': 2}. Best is trial 13 with value: 1.498906284570694.\n",
            "[I 2025-08-20 07:07:17,517] Trial 14 finished with value: 2.3913958370685577 and parameters: {'lr': 0.002577832821353061, 'weight_decay': 3.6298190241563296e-06, 'dropout': 0.16764142443400415, 'mfs_per_feature': 2}. Best is trial 13 with value: 1.498906284570694.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'lr': 0.009441711029875932, 'weight_decay': 1.0215468948926014e-06, 'dropout': 0.19981367458227509, 'mfs_per_feature': 2}\n",
            "Training enhanced model...\n",
            "Epoch 0001 | Train Loss: 5.3254 | Val Loss: 3.4594 | LR: 9.44e-03\n",
            "Epoch 0020 | Train Loss: 3.1659 | Val Loss: 1.5977 | LR: 9.44e-03\n",
            "Epoch 0040 | Train Loss: 2.7028 | Val Loss: 1.2957 | LR: 9.44e-03\n",
            "Epoch 0060 | Train Loss: 2.4195 | Val Loss: 1.2392 | LR: 9.44e-03\n",
            "Epoch 0080 | Train Loss: 2.4696 | Val Loss: 1.3223 | LR: 4.72e-03\n",
            "Epoch 0100 | Train Loss: 2.2158 | Val Loss: 1.3834 | LR: 2.36e-03\n",
            "Early stopping at epoch 103\n",
            "Training completed in 0.1 minutes\n",
            "\n",
            "=== Final Results ===\n",
            "Metrics (Train) {\n",
            "  \"RMSE\": 9.092879525697438,\n",
            "  \"MAE\": 2.3885867595672607,\n",
            "  \"R2\": 0.5463216304779053,\n",
            "  \"MAPE_%\": 82.64302062988281\n",
            "}\n",
            "Metrics (Val)   {\n",
            "  \"RMSE\": 6.863434096725753,\n",
            "  \"MAE\": 1.741745114326477,\n",
            "  \"R2\": 0.6170979738235474,\n",
            "  \"MAPE_%\": 183.55560302734375\n",
            "}\n",
            "Metrics (Test)  {\n",
            "  \"RMSE\": 11.044556974796738,\n",
            "  \"MAE\": 3.5072827339172363,\n",
            "  \"R2\": 0.518256664276123,\n",
            "  \"MAPE_%\": 67.61935424804688\n",
            "}\n",
            "\n",
            "All artifacts saved to: /content/drive/MyDrive/NNsGA/FNNs/artifacts_enhanced\n"
          ]
        }
      ],
      "source": [
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_excel(DATA_CSV)\n",
        "\n",
        "    # Verify all columns exist\n",
        "    for col in FEATURE_COLS + [TARGET_COL]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Missing column: {col}\")\n",
        "\n",
        "    print(\"Data shape:\", df.shape)\n",
        "    print(\"Features:\", FEATURE_COLS)\n",
        "\n",
        "    X = df[FEATURE_COLS].values.astype(np.float32)\n",
        "    y = df[TARGET_COL].values.astype(np.float32)\n",
        "\n",
        "    # Handle extreme values in target\n",
        "    y = np.clip(y, np.percentile(y, 1), np.percentile(y, 99))\n",
        "\n",
        "    # Split data (70/15/15)\n",
        "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=SEED)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=SEED)\n",
        "\n",
        "    # Scale features (using all 6 features)\n",
        "    scaler    = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_val_s   = scaler.transform(X_val)\n",
        "    X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Training set: {X_train_s.shape}, Validation set: {X_val_s.shape}, Test set: {X_test_s.shape}\")\n",
        "\n",
        "    # Hyperparameter optimization\n",
        "    print(\"Optimizing hyperparameters...\")\n",
        "    best_params = optimize_hyperparameters(X_train_s, y_train, X_val_s, y_val)\n",
        "\n",
        "    # Create final config\n",
        "    cfg = EnhancedTrainConfig(\n",
        "        lr=best_params['lr'],\n",
        "        weight_decay=best_params['weight_decay'],\n",
        "        dropout_rate=best_params['dropout'],\n",
        "        mfs_per_feature=best_params['mfs_per_feature']\n",
        "    )\n",
        "\n",
        "    # Build and initialize model\n",
        "    D = X_train_s.shape[1]  # Should be 6 (all features)\n",
        "    model = EnhancedTSKFuzzyRegressor(D, cfg.mfs_per_feature, cfg)\n",
        "    enhanced_init_mfs(model, X_train_s)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_ds = TabDataset(X_train_s, y_train)\n",
        "    val_ds   = TabDataset(X_val_s, y_val)\n",
        "    test_ds  = TabDataset(X_test_s, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training enhanced model...\")\n",
        "    start_time = time.time()\n",
        "    history = enhanced_train_model(model, train_loader, val_loader, cfg)\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training completed in {training_time/60:.1f} minutes\")\n",
        "\n",
        "    # Evaluate\n",
        "    train_metrics, yhat_train = evaluate(model, X_train_s, y_train)\n",
        "    val_metrics, yhat_val = evaluate(model, X_val_s, y_val)\n",
        "    test_metrics, yhat_test = evaluate(model, X_test_s, y_test)\n",
        "\n",
        "    # Convert to JSON serializable\n",
        "    train_metrics_json = {k: float(v) for k, v in train_metrics.items()}\n",
        "    val_metrics_json   = {k: float(v) for k, v in val_metrics.items()}\n",
        "    test_metrics_json  = {k: float(v) for k, v in test_metrics.items()}\n",
        "\n",
        "    print(\"\\n=== Final Results ===\")\n",
        "    print(\"Metrics (Train)\", json.dumps(train_metrics_json, indent=2))\n",
        "    print(\"Metrics (Val)  \", json.dumps(val_metrics_json, indent=2))\n",
        "    print(\"Metrics (Test) \", json.dumps(test_metrics_json, indent=2))\n",
        "\n",
        "    # Save results and plots\n",
        "    plot_training(history, ARTIFACTS_DIR)\n",
        "    plot_parity(y_train, yhat_train, ARTIFACTS_DIR, \"Train\")\n",
        "    plot_parity(y_val, yhat_val, ARTIFACTS_DIR, \"Validation\")\n",
        "    plot_parity(y_test, yhat_test, ARTIFACTS_DIR, \"Test\")\n",
        "\n",
        "    # Save model and results\n",
        "    torch.save({\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"config\": cfg.__dict__,\n",
        "        \"feature_cols\": FEATURE_COLS,\n",
        "        \"scaler_mean_\": scaler.mean_.tolist(),\n",
        "        \"scaler_scale_\": scaler.scale_.tolist(),\n",
        "    }, os.path.join(ARTIFACTS_DIR, \"enhanced_tsk_model.pt\"))\n",
        "\n",
        "    print(f\"\\nAll artifacts saved to: {ARTIFACTS_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
